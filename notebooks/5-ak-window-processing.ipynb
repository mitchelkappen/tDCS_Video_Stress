{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the windows\n",
    "This notebook focusses on the processing of the sampled windows, to prepare them to be used as training and test data in the modelling part. At this point it only involves checking how many participants are still included in the windows DataFrames and adding the age and gender information to all the windows. More processing steps, could be added to this notebook.\n",
    "#### Requirements\n",
    "If one wants to run this notebook, make sure that you have run the `4-ak-window-sampling` notebook. This notebook is responsible for processing the windows DataFrames, stored in `data\\processed`, created by this previous notebook.\n",
    "You also need to have stored the demographics file `Demogr tDCS WM stress.csv` in the `data\\raw` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mitchel = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading in the required modules, we store the working directory in a variable called `project_dir`. We then store the folder where the data files are located in a variable called `data_dir`. We also store the specific processed directory and processed files in specific directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.getcwd().split('\\\\')[:-1]\n",
    "project_dir = '\\\\'.join(project_dir) # Get the project dir \n",
    "# Get the data dir\n",
    "if Mitchel:\n",
    "    data_dir = 'C:\\\\Users\\\\mitch\\\\OneDrive - UGent\\\\UGent\\\\Projects\\\\7. tDCS_Stress_WM_deSmet\\\\data'\n",
    "    data_dir = 'Z:\\\\ghep_lab\\\\2020_DeSmetKappen_tDCS_Stress_WM_VIDEO\\\\Data'\n",
    "else: \n",
    "    data_dir = project_dir + '\\\\data'\n",
    "processed_dir = data_dir + '\\\\processed' # Get the processed subdir\n",
    "processed_files = [file for file in os.listdir(processed_dir) if file.endswith('feather') and not file.startswith('processed_') and not file.startswith('RESULTS')] # Get the processed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "Below we execute some simple processing steps, if necessary more can be added below.\n",
    "### Outlier removal\n",
    "Below we remove the segments that have an outlying value in of the two target variables. This is executed for both data files and then written back to the original feather file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def remove_outliers(data, targets):\n",
    "    return data[(np.abs(stats.zscore(data[targets])) < 3).all(axis=1)]\n",
    "\n",
    "for file in processed_files:\n",
    "    window = pd.read_feather(f'{processed_dir}\\\\{file}')\n",
    "    before = len(window.index)\n",
    "    window = remove_outliers(window, ['standardised_mean_scl', 'HRV_SDNN_corrected'])\n",
    "    after = len(window.index)\n",
    "    print(f'Removed {before-after} segments from {file}')\n",
    "    window.reset_index(inplace=True, drop=True)\n",
    "    window.to_feather(f'{processed_dir}\\\\processed_{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how many participants were still in the data sample\n",
    "We noticed that we did not extract data from all participants. The first cell shows from how many participants windows were present in the different DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in processed_files:\n",
    "    window = pd.read_feather(f'{processed_dir}\\\\{file}')\n",
    "    print(f'In file {file} there is data from {len(window.pp.unique())} participants')\n",
    "    print(window.pp.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender & Age\n",
    "In the next two cells we add the age and gender of the participant to each processed window. We get the age and gender from the demographics file `Demogr tDCS WM stress.csv`. We also applied a quick fix since some rows did not separate properly. In the second cell we loop over the DataFrames, adding the gender and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demogr = pd.read_csv(data_dir + '\\\\raw\\\\Demogr tDCS WM stress.csv', sep=',') # Reading in the demographics DataFrame\n",
    "\n",
    "## Handling the rows that did not separatly properly\n",
    "i = demogr[demogr.id.str.split(',').str.len()>2].index ## Finding the rows that did not seperate by checking if they now do split\n",
    "demogr.loc[i, 'geboortedatum_patient'] = demogr.loc[i, 'id'].str.split(',').str[1] # Extracting the birth date, and adding this to the column geboortedatum_patient\n",
    "demogr.loc[i, 'geslacht'] = demogr.loc[i, 'id'].str.split(',').str[2] # Extracting the gender, and adding this to the column geslacht\n",
    "demogr.loc[i, 'id'] = demogr.loc[i, 'id'].str.split(',').str[0] # Extracting the id, and adding this to the column id\n",
    "\n",
    "demogr = demogr[['id', 'geboortedatum_patient', 'geslacht']] # Dropping all the other columns from the DataFrame\n",
    "demogr['geboortedatum_patient'] = pd.to_datetime(demogr['geboortedatum_patient']) # Setting the birth date column as a datetime object\n",
    "now = pd.Timestamp('now') # Getting the current time\n",
    "demogr['leeftijd'] = (now - demogr['geboortedatum_patient']).astype('<m8[Y]') # Subtracting the birth dates from the current time to get the age of the participant in years.\n",
    "demogr[['id']] = demogr[['id']].astype('str')\n",
    "demogr[['geslacht']] = demogr[['geslacht']].astype('int')\n",
    "demogr[['leeftijd']] = demogr[['leeftijd']].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demogr.id = demogr.id.str.split(',').str[0].astype('int64') # Setting the id of the participant as int type (necessary to merge on pp id)\n",
    "\n",
    "for file in processed_files: # For each windows DataFrame\n",
    "    window = pd.read_feather(f'{processed_dir}\\\\processed_{file}') # Read in the windows DataFrame\n",
    "    window.pp = window.pp.astype('int64') # Set pp id as int type\n",
    "    df = window.merge(demogr[['id', 'geslacht', 'leeftijd']], how='left', left_on='pp', right_on='id', validate='many_to_one') # Adding the age and gender of each participant (merging on pp_id)\n",
    "    df.to_feather(f'{processed_dir}\\\\processed_{file}') # Saving the DataFrame in the processed dir with the prefix \"processed_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "The cell below is responsible for creating the graphs that are presented in the results section of the final paper.import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = pd.read_feather(f'{processed_dir}\\\\processed_window_180_step_180.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "target = 'Standardised SCL'\n",
    "ax.hist(window['standardised_mean_scl'], bins=20)\n",
    "ax.set_title(f'Distribution of {target} (N={len(window[\"standardised_mean_scl\"])})')\n",
    "ax.set_xlabel(target)\n",
    "ax.set_ylabel('Frequency')\n",
    "textstr = f'\\u03BC: {window[\"standardised_mean_scl\"].mean():.3f}\\n\\u03C3: {window[\"standardised_mean_scl\"].std():.3f}'\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "plt.savefig(f'{project_dir}\\\\reports\\\\figures\\\\distribution_{target}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "target = 'Corrected SDNN'\n",
    "ax.hist(window['HRV_SDNN_corrected'], bins=20)\n",
    "ax.set_title(f'Distribution of {target} (N={len(window[\"HRV_SDNN_corrected\"])})')\n",
    "ax.set_xlabel(target)\n",
    "ax.set_ylabel('Frequency')\n",
    "textstr = f'\\u03BC: {window[\"HRV_SDNN_corrected\"].mean():.3f}\\n\\u03C3: {window[\"HRV_SDNN_corrected\"].std():.3f}'\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "plt.savefig(f'{project_dir}\\\\reports\\\\figures\\\\distribution_{target}.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
