{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling of the dataset\n",
    "This notebook implements the sampling of the dataset, running a sliding window with a fixed timestep over the video and physio dataframe of each subject. For each sample we can compute a set of features and target based on the raw information that is in that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from targetComputation import compute_EDA_Targets, compute_ECG_Targets\n",
    "import features as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.getcwd().split('\\\\')[:-1]\n",
    "project_dir = '\\\\'.join(project_dir)\n",
    "data_dir = project_dir + '\\\\data'\n",
    "video_dir = data_dir+'\\\\interim\\\\video'\n",
    "physio_dir = data_dir+'\\\\interim\\\\physiological'\n",
    "video_files = [file for file in os.listdir(video_dir)]\n",
    "physio_files = [file for file in os.listdir(physio_dir)]\n",
    "pps = list(set([file[:-4] for file in video_files]).intersection(set([file[:-4] for file in physio_files])))\n",
    "pps = sorted([int(pp) for pp in pps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for 66 participants.\n"
     ]
    }
   ],
   "source": [
    "print(f'Collecting data for {len(pps)} participants.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and end point of samples\n",
    "After loading the required modules and storing the different location of the data files in variables, we implemented some functions that will perform the sampling below. First off the `get_start_end` function. This function finds all the start and end points of a sample based on a video signal. You can input a desired window size and step size and this function will compute the start and end points of the all the samples that can be sampled from the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end(data, window_size, step_size):\n",
    "    \"\"\"Get start & end timestamps of a video signal based on a given window size and step size.\n",
    "    Returns a tuple (start, end frame)\n",
    "    \"\"\"\n",
    "\n",
    "    points = []\n",
    "    start = data.t_from_start.values[0]\n",
    "    end = start + window_size\n",
    "    points.append((start, end))\n",
    "\n",
    "    while end+step_size < data.t_from_start.values[-1]:\n",
    "        start += step_size\n",
    "        end += step_size\n",
    "        points.append((start, end))\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting one window\n",
    "The function `get_window` grabs one window/sample from a dataframe, based on a given start and end point. Combined with the `get_start_end` function, we can gather all the possible windows/samples from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(data, start, end):\n",
    "    \"\"\"Return window between a certain start and endpoint.\"\"\"\n",
    "    return data.loc[(data.t_from_start >= start) & (data.t_from_start <= end), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing a window\n",
    "Below we have implemented two functions for the processing of respectively a video sample and a physiological window. The functions `process_physio_window` and `process_video_window` have as input a window/sample (a subset of dataframe obtained through the function `get_window`) and compute various features or targets, which are returned in a dict. \n",
    "\n",
    "When we want to compute extra or other features/targets, we can add those in these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_physio_window(window):\n",
    "    \"\"\"Processes the physiological window. Returns a dict containing the computed target variables and the first and last timestamp of the window.\"\"\"\n",
    "    processed_window = {}\n",
    "    \n",
    "    processed_window['t_start_physio'] = window.t_from_start.values[0]\n",
    "    processed_window['t_end_physio'] = window.t_from_start.values[-1]\n",
    "    \n",
    "    processed_window = {**processed_window, **compute_EDA_Targets(window)}\n",
    "    processed_window = {**processed_window, **compute_ECG_Targets(window)}\n",
    "    \n",
    "    \n",
    "    return processed_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_window(window):\n",
    "    \"\"\"Processes the video window. Returns a dict containing the computed features and the first and last frame of the window.\"\"\"\n",
    "    processed_window = {}\n",
    "    \n",
    "    processed_window['proportion_Success'] = window.success.sum()/len(window.success)\n",
    "    processed_window['t_start_video'] = window.t_from_start.values[0]\n",
    "    processed_window['t_end_video'] = window.t_from_start.values[-1]    \n",
    "    processed_window = {**processed_window, **ft.compute_mean_AUs(window)}\n",
    "    processed_window = {**processed_window, **ft.compute_std_AUs(window)}\n",
    "    processed_window = {**processed_window, **ft.compute_arousal(window)}\n",
    "    processed_window = {**processed_window, **ft.compute_emotions(window)}\n",
    "    processed_window = {**processed_window, **ft.compute_head_motion(window)}\n",
    "    processed_window = {**processed_window, **ft.compute_PD_features(window)}\n",
    "    processed_window['blink_rate'] = ft.compute_blink_rate(window)\n",
    "    processed_window['per_EC'] = ft.compute_percentage_EC(window)\n",
    "    \n",
    "    return processed_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_video_window(window):\n",
    "    \"\"\"Checks the video window, based on the defined rules. If one rule is broken we return false and do not use the entire window. Else we return true\"\"\"\n",
    "    global removed\n",
    "    if (window.confidence >= 0.8).sum()/len(window.confidence) < 0.95:\n",
    "        removed += 1\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and processing one participant\n",
    "Below we have implemented the `sample_pp` function, which samples and processes the data of one participant. After reading in the physio and video dataframe of the participant, it implements the four functions from earlier to sample and process those dataframes. It stores the processed variables in a dict, which it returns for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pp(physio_data, video_data, window_size, step_size):\n",
    "    \"\"\"Samples the data of one specific participant. Make sure that the both the physiological and video data exists for the participant.\"\"\"\n",
    "    points = get_start_end(video_data, window_size, step_size)\n",
    "    processed_windows = []\n",
    "    i = 1\n",
    "    for point in points:\n",
    "        start, end = point\n",
    "        video_window = get_window(video_data, start, end)\n",
    "        if check_video_window(video_window):                \n",
    "            physio_window = get_window(physio_data, start, end)\n",
    "            \n",
    "            processed_physio_window = process_physio_window(physio_window)\n",
    "            processed_video_window = process_video_window(video_window)\n",
    "\n",
    "            processed_window = {**processed_video_window, **processed_physio_window}\n",
    "            processed_window['start'] = start\n",
    "            processed_window['end'] = end\n",
    "            processed_window['pp'] = pp\n",
    "            processed_window['pp_window'] = i\n",
    "            \n",
    "            processed_windows.append(processed_window)\n",
    "        i += 1\n",
    "    return processed_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the participant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pp:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp_physio = {}\n",
    "pp_video = {}\n",
    "for pp in tqdm(pps, desc='pp', leave=False):\n",
    "    pp_physio[pp] = pd.read_hdf(f'{physio_dir}\\\\{pp}.hdf', f'pp{pp}')\n",
    "    df = pd.read_hdf(f'{video_dir}\\\\{pp}.hdf', f'pp{pp}')\n",
    "    cols = [col for col in df.columns if col not in ['frame', 'face_id', 'timestamp', 'confidence', 'success', 'started', 'pp', 't_from_start', 'frames_away_start ']]\n",
    "    df.loc[df.confidence<0.8, cols] = np.nan\n",
    "    pp_video[pp] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing all the participants\n",
    "Below we have sampled all the participants and stored their processed variables in a dataframe, which is going to be the dataframe that we will be using in the modelling step. Below you can adjust the desired window_size and step_size. The dataset is stored in the desired location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e00220bde54310b0a70c0dbb5ad6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "windows:   0%|          | 0/2 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "steps:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pp:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished window size: 120 step size: 120. Sampled a total of 301 windows. Removed total of 221 windows, ~42 percent of all possible windows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pp:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished window size: 120 step size: 108. Sampled a total of 320 windows. Removed total of 260 windows, ~44 percent of all possible windows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pp:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished window size: 120 step size: 84. Sampled a total of 428 windows. Removed total of 312 windows, ~42 percent of all possible windows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "steps:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pp:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished window size: 180 step size: 180. Sampled a total of 178 windows. Removed total of 153 windows, ~46 percent of all possible windows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pp:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished window size: 180 step size: 162. Sampled a total of 180 windows. Removed total of 198 windows, ~52 percent of all possible windows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pp:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished window size: 180 step size: 125. Sampled a total of 254 windows. Removed total of 217 windows, ~46 percent of all possible windows.\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('mode.chained_assignment',None)\n",
    "window_sizes = [60*2, 60*3]\n",
    "step_sizes = [1, 0.9, 0.7]\n",
    "\n",
    "for window_size in tqdm(window_sizes, desc='windows'):\n",
    "    for step_size in tqdm(step_sizes, desc='steps', leave=False):\n",
    "        df = []\n",
    "        step_size *= window_size\n",
    "        step_size = int(step_size)\n",
    "        removed = 0\n",
    "        for pp in tqdm(pps, desc='pp', leave=False):\n",
    "            df += sample_pp(pp_physio[pp], pp_video[pp], window_size, step_size)\n",
    "        df = pd.DataFrame(df)\n",
    "        df.pp = df['pp'].astype('str')\n",
    "        df['window'] = df.index\n",
    "        print(f'Finished window size: {window_size} step size: {step_size}. Sampled a total of {len(df.pp)} windows. Removed total of {removed} windows, ~{int(((removed)/(removed+len(df.pp)))*100)} percent of all possible windows.')\n",
    "        df.to_hdf(f\"{data_dir}\\\\processed\\\\window_{window_size}_step_{step_size}.hdf\", \"data\", mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
